{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alanjamlu34/Web-Scraping-IMDB-Movie-Reviews/blob/main/XTTS_FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th91ofnQWr8Y"
      },
      "source": [
        "## Dataset building + XTTS finetuning and inference\n",
        "\n",
        "#### Running the demo\n",
        "To start the demo run the first two cells (ignore pip install errors in the first one)\n",
        "\n",
        "Then click on the link `Running on public URL: ` when the demo is ready.\n",
        "\n",
        "#### Downloading the results\n",
        "\n",
        "You can run cell [3] to zip and download default dataset path\n",
        "\n",
        "You can run cell [4] to zip and download the latest model you trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdWKA_xFqkKq"
      },
      "source": [
        "### Installing the requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lmUUQqdN6BXk",
        "outputId": "d30432f8-d8a9-4100-9174-1b6e408ecc3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for TTS (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's legacy dependency resolver does not consider dependency conflicts when selecting packages. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you'll have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you'll have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you'll have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you'll have pandas 1.5.3 which is incompatible.\n",
            "librosa 0.10.2.post1 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you'll have numpy 1.22.0 which is incompatible.\n",
            "numexpr 2.10.1 requires numpy>=1.23.0, but you'll have numpy 1.22.0 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you'll have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you'll have numpy 1.22.0 which is incompatible.\n",
            "pywavelets 1.6.0 requires numpy<3,>=1.22.4, but you'll have numpy 1.22.0 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you'll have numpy 1.22.0 which is incompatible.\n",
            "statsmodels 0.14.2 requires numpy>=1.22.3, but you'll have numpy 1.22.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you'll have numpy 1.22.0 which is incompatible.\n",
            "gruut 2.2.3 requires networkx<3.0.0,>=2.5.0, but you'll have networkx 3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!rm -rf TTS/ # delete repo to be able to reinstall if needed\n",
        "!git clone --branch xtts_demo -q https://github.com/coqui-ai/TTS.git\n",
        "!pip install --use-deprecated=legacy-resolver -q -e TTS\n",
        "#!pip install --use-deprecated=legacy-resolver -q -r TTS/TTS/demos/xtts_ft_demo/requirements.txt\n",
        "#!pip install -q typing_extensions==4.8 numpy==1.26.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7rNt1e2qtDP"
      },
      "source": [
        "### Running the gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "kzJk5wTziz4M",
        "outputId": "fed3c5da-8830-47de-a295-131ff06d226b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faster_whisper"
      ],
      "metadata": {
        "id": "z09qMqJzbsmo",
        "outputId": "b812b2d5-259e-4870-a472-1abab3ca960d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd2xo_7a8wyj",
        "outputId": "fa2a53e5-5515-4572-f716-d34d35c52560",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-15 04:13:15,589 [INFO] HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
            "2024-07-15 04:13:15,593 [INFO] HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
            "Running on local URL:  http://0.0.0.0:5003\n",
            "2024-07-15 04:13:15,700 [INFO] HTTP Request: GET http://localhost:5003/startup-events \"HTTP/1.1 200 OK\"\n",
            "2024-07-15 04:13:15,722 [INFO] HTTP Request: HEAD http://localhost:5003/ \"HTTP/1.1 200 OK\"\n",
            "2024-07-15 04:13:15,940 [INFO] HTTP Request: GET https://api.gradio.app/v2/tunnel-request \"HTTP/1.1 200 OK\"\n",
            "Running on public URL: https://77eb18f4a1c9bc5508.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Loading Whisper Model!\n",
            "2024-07-15 04:14:08,308 [INFO] Processing audio with duration 17:37.460\n",
            "Dataset Processed!\n",
            ">> DVAE weights restored from: /tmp/xtts_ft/run/training/XTTS_v2.0_original_model_files/dvae.pth\n",
            " | > Found 233 files in /tmp/xtts_ft/dataset\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            " > Training Environment:\n",
            " | > Backend: Torch\n",
            " | > Mixed precision: False\n",
            " | > Precision: float32\n",
            " | > Current device: 0\n",
            " | > Num. of GPUs: 1\n",
            " | > Num. of CPUs: 2\n",
            " | > Num. of Torch Threads: 1\n",
            " | > Torch seed: 1\n",
            " | > Torch CUDNN: True\n",
            " | > Torch CUDNN deterministic: False\n",
            " | > Torch CUDNN benchmark: False\n",
            " | > Torch TF32 MatMul: False\n",
            "2024-07-15 04:17:04.727081: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-15 04:17:04.727139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-15 04:17:04.732863: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-15 04:17:04.754006: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-15 04:17:06.867072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            " > Start Tensorboard: tensorboard --logdir=/tmp/xtts_ft/run/training/GPT_XTTS_FT-July-15-2024_04+17AM-0000000\n",
            "\n",
            " > Model has 518442047 parameters\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 0/6\u001b[0m\n",
            " --> /tmp/xtts_ft/run/training/GPT_XTTS_FT-July-15-2024_04+17AM-0000000\n",
            " > Sampling by language: dict_keys(['en'])\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2024-07-15 04:17:09) \u001b[0m\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:17:20 -- STEP: 0/117 -- GLOBAL_STEP: 0\u001b[0m\n",
            "     | > loss_text_ce: 0.02343878149986267  (0.02343878149986267)\n",
            "     | > loss_mel_ce: 4.180198669433594  (4.180198669433594)\n",
            "     | > loss: 4.203637599945068  (4.203637599945068)\n",
            "     | > grad_norm: 0  (0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 1.3684  (1.3684024810791016)\n",
            "     | > loader_time: 9.8752  (9.875202417373657)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:17:49 -- STEP: 50/117 -- GLOBAL_STEP: 50\u001b[0m\n",
            "     | > loss_text_ce: 0.021836528554558754  (0.02337367985397577)\n",
            "     | > loss_mel_ce: 4.087556838989258  (3.9607550144195556)\n",
            "     | > loss: 4.10939359664917  (3.9841287136077903)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2608  (0.2848899507522582)\n",
            "     | > loader_time: 0.0602  (0.03034690856933593)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:18:18 -- STEP: 100/117 -- GLOBAL_STEP: 100\u001b[0m\n",
            "     | > loss_text_ce: 0.0211495291441679  (0.02325441574677825)\n",
            "     | > loss_mel_ce: 3.905722141265869  (3.8074521923065188)\n",
            "     | > loss: 3.9268717765808105  (3.8307066082954417)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.3072  (0.2856031560897827)\n",
            "     | > loader_time: 0.0125  (0.026190316677093504)\n",
            "\n",
            " > Filtering invalid eval samples!!\n",
            " > Total eval samples after filtering: 41\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.05953514575958252 \u001b[0m(+0)\n",
            "     | > avg_loss_text_ce: 0.021137213613837958 \u001b[0m(+0)\n",
            "     | > avg_loss_mel_ce: 3.5719672203063966 \u001b[0m(+0)\n",
            "     | > avg_loss: 3.5931044220924377 \u001b[0m(+0)\n",
            "\n",
            " > BEST MODEL : /tmp/xtts_ft/run/training/GPT_XTTS_FT-July-15-2024_04+17AM-0000000/best_model_117.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 1/6\u001b[0m\n",
            " --> /tmp/xtts_ft/run/training/GPT_XTTS_FT-July-15-2024_04+17AM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-07-15 04:21:17) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:21:38 -- STEP: 33/117 -- GLOBAL_STEP: 150\u001b[0m\n",
            "     | > loss_text_ce: 0.023642096668481827  (0.02226983022057649)\n",
            "     | > loss_mel_ce: 3.9107697010040283  (3.5231178601582847)\n",
            "     | > loss: 3.9344117641448975  (3.5453876798803154)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1442  (0.29502818801186304)\n",
            "     | > loader_time: 0.0089  (0.02524375193046801)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:22:05 -- STEP: 83/117 -- GLOBAL_STEP: 200\u001b[0m\n",
            "     | > loss_text_ce: 0.02518853358924389  (0.022732189373977217)\n",
            "     | > loss_mel_ce: 2.768787384033203  (3.399862680090479)\n",
            "     | > loss: 2.793975830078125  (3.4225948661206718)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1823  (0.28119119965886497)\n",
            "     | > loader_time: 0.0084  (0.018572606236101633)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.05404001474380493 \u001b[0m(-0.005495131015777588)\n",
            "     | > avg_loss_text_ce:\u001b[92m 0.020782764069736004 \u001b[0m(-0.0003544495441019542)\n",
            "     | > avg_loss_mel_ce:\u001b[92m 3.4913842558860777 \u001b[0m(-0.08058296442031887)\n",
            "     | > avg_loss:\u001b[92m 3.5121670007705688 \u001b[0m(-0.08093742132186899)\n",
            "\n",
            " > BEST MODEL : /tmp/xtts_ft/run/training/GPT_XTTS_FT-July-15-2024_04+17AM-0000000/best_model_234.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 2/6\u001b[0m\n",
            " --> /tmp/xtts_ft/run/training/GPT_XTTS_FT-July-15-2024_04+17AM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-07-15 04:24:36) \u001b[0m\n",
            "[!] Warning: The text length exceeds the character limit of 250 for language 'en', this might cause truncated audio.\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:24:47 -- STEP: 16/117 -- GLOBAL_STEP: 250\u001b[0m\n",
            "     | > loss_text_ce: 0.018711654469370842  (0.022197839338332415)\n",
            "     | > loss_mel_ce: 3.5461108684539795  (3.352487877011299)\n",
            "     | > loss: 3.5648224353790283  (3.3746857196092606)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2304  (0.283918172121048)\n",
            "     | > loader_time: 0.0548  (0.018895938992500305)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:25:16 -- STEP: 66/117 -- GLOBAL_STEP: 300\u001b[0m\n",
            "     | > loss_text_ce: 0.02510225772857666  (0.022388980300589043)\n",
            "     | > loss_mel_ce: 3.148768901824951  (3.31173308690389)\n",
            "     | > loss: 3.1738710403442383  (3.3341220545046255)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.3042  (0.28392193534157506)\n",
            "     | > loader_time: 0.0103  (0.01976232095198198)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:25:44 -- STEP: 116/117 -- GLOBAL_STEP: 350\u001b[0m\n",
            "     | > loss_text_ce: 0.024795524775981903  (0.02273287617697798)\n",
            "     | > loss_mel_ce: 2.480135917663574  (3.29590681503559)\n",
            "     | > loss: 2.5049314498901367  (3.3186396833123832)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1467  (0.27817381455980483)\n",
            "     | > loader_time: 0.0251  (0.017370774828154464)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.05291060209274292 \u001b[0m(-0.001129412651062013)\n",
            "     | > avg_loss_text_ce:\u001b[92m 0.02059951815754175 \u001b[0m(-0.0001832459121942527)\n",
            "     | > avg_loss_mel_ce:\u001b[92m 3.4477720975875856 \u001b[0m(-0.043612158298492076)\n",
            "     | > avg_loss:\u001b[92m 3.468371641635895 \u001b[0m(-0.043795359134673806)\n",
            "\n",
            " > BEST MODEL : /tmp/xtts_ft/run/training/GPT_XTTS_FT-July-15-2024_04+17AM-0000000/best_model_351.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 3/6\u001b[0m\n",
            " --> /tmp/xtts_ft/run/training/GPT_XTTS_FT-July-15-2024_04+17AM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-07-15 04:28:27) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:28:56 -- STEP: 49/117 -- GLOBAL_STEP: 400\u001b[0m\n",
            "     | > loss_text_ce: 0.026577824726700783  (0.022572782993012547)\n",
            "     | > loss_mel_ce: 2.9481284618377686  (3.1545080019503224)\n",
            "     | > loss: 2.9747061729431152  (3.177080772360977)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2949  (0.26469116794819736)\n",
            "     | > loader_time: 0.0095  (0.01807096539711465)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:29:23 -- STEP: 99/117 -- GLOBAL_STEP: 450\u001b[0m\n",
            "     | > loss_text_ce: 0.02108418196439743  (0.022301753437278245)\n",
            "     | > loss_mel_ce: 2.7364134788513184  (3.092420948876275)\n",
            "     | > loss: 2.757497549057007  (3.1147226877886838)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2807  (0.26336936998848964)\n",
            "     | > loader_time: 0.0094  (0.015415018255060376)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0531819224357605 \u001b[0m(+0.0002713203430175781)\n",
            "     | > avg_loss_text_ce:\u001b[92m 0.020438903663307428 \u001b[0m(-0.0001606144942343235)\n",
            "     | > avg_loss_mel_ce:\u001b[92m 3.43938353061676 \u001b[0m(-0.008388566970825462)\n",
            "     | > avg_loss:\u001b[92m 3.4598224401474 \u001b[0m(-0.00854920148849514)\n",
            "\n",
            " > BEST MODEL : /tmp/xtts_ft/run/training/GPT_XTTS_FT-July-15-2024_04+17AM-0000000/best_model_468.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 4/6\u001b[0m\n",
            " --> /tmp/xtts_ft/run/training/GPT_XTTS_FT-July-15-2024_04+17AM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-07-15 04:32:24) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:32:46 -- STEP: 32/117 -- GLOBAL_STEP: 500\u001b[0m\n",
            "     | > loss_text_ce: 0.020639579743146896  (0.021029695635661483)\n",
            "     | > loss_mel_ce: 3.101627826690674  (3.102634981274605)\n",
            "     | > loss: 3.122267484664917  (3.123664677143097)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1809  (0.3089299947023392)\n",
            "     | > loader_time: 0.0093  (0.01829206943511963)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:33:14 -- STEP: 82/117 -- GLOBAL_STEP: 550\u001b[0m\n",
            "     | > loss_text_ce: 0.02060049958527088  (0.02138436622009044)\n",
            "     | > loss_mel_ce: 2.9214842319488525  (3.030373471539195)\n",
            "     | > loss: 2.942084789276123  (3.0517578386678923)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2847  (0.2955954016708746)\n",
            "     | > loader_time: 0.0106  (0.01524710364458037)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.053343677520751955 \u001b[0m(+0.00016175508499145785)\n",
            "     | > avg_loss_text_ce:\u001b[92m 0.020367935579270125 \u001b[0m(-7.096808403730254e-05)\n",
            "     | > avg_loss_mel_ce:\u001b[91m 3.4442496299743652 \u001b[0m(+0.004866099357605069)\n",
            "     | > avg_loss:\u001b[91m 3.4646175622940065 \u001b[0m(+0.004795122146606712)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 5/6\u001b[0m\n",
            " --> /tmp/xtts_ft/run/training/GPT_XTTS_FT-July-15-2024_04+17AM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-07-15 04:33:37) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:33:48 -- STEP: 15/117 -- GLOBAL_STEP: 600\u001b[0m\n",
            "     | > loss_text_ce: 0.020277179777622223  (0.02164971878131231)\n",
            "     | > loss_mel_ce: 2.9931392669677734  (3.0750441869099934)\n",
            "     | > loss: 3.0134165287017822  (3.096693929036458)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2998  (0.2870556672414144)\n",
            "     | > loader_time: 0.031  (0.013685051600138347)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:34:18 -- STEP: 65/117 -- GLOBAL_STEP: 650\u001b[0m\n",
            "     | > loss_text_ce: 0.018106581643223763  (0.021708932518959043)\n",
            "     | > loss_mel_ce: 2.7582952976226807  (3.0019258572505074)\n",
            "     | > loss: 2.776401996612549  (3.0236347932081955)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.254  (0.2994049879220816)\n",
            "     | > loader_time: 0.0253  (0.016803363653329698)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-07-15 04:34:47 -- STEP: 115/117 -- GLOBAL_STEP: 700\u001b[0m\n",
            "     | > loss_text_ce: 0.022021668031811714  (0.021797351595824185)\n",
            "     | > loss_mel_ce: 3.4368512630462646  (2.9717350234156075)\n",
            "     | > loss: 3.4588730335235596  (2.9935323777406113)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.3786  (0.29458446710006037)\n",
            "     | > loader_time: 0.0112  (0.01574905436971913)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.05400415658950806 \u001b[0m(+0.0006604790687561035)\n",
            "     | > avg_loss_text_ce:\u001b[92m 0.02031207475811243 \u001b[0m(-5.586082115769525e-05)\n",
            "     | > avg_loss_mel_ce:\u001b[91m 3.465640938282013 \u001b[0m(+0.021391308307647794)\n",
            "     | > avg_loss:\u001b[91m 3.4859530448913576 \u001b[0m(+0.021335482597351074)\n",
            "\n",
            "Model training done!\n",
            "Loading XTTS model! \n",
            "Model Loaded!\n",
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:1466: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  resampled = torch.nn.functional.conv1d(waveform[:, None], kernel, stride=orig_freq)\n"
          ]
        }
      ],
      "source": [
        "!python TTS/TTS/demos/xtts_ft_demo/xtts_demo.py --batch_size 2 --num_epochs 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXEBRA_kq23i"
      },
      "source": [
        "### Downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBxgdKcvi4kO"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "!zip -q -r dataset.zip /tmp/xtts_ft/dataset\n",
        "files.download('dataset.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKzoP53Nq_rJ"
      },
      "source": [
        "### Downloading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpfdzHvKaX8M"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "def find_latest_best_model(folder_path):\n",
        "    search_path = os.path.join(folder_path, '**', 'best_model.pth')\n",
        "    files = glob.glob(search_path, recursive=True)\n",
        "    latest_file = max(files, key=os.path.getctime, default=None)\n",
        "    return latest_file\n",
        "\n",
        "model_path = find_latest_best_model(\"/tmp/xtts_ft/run/training/\")\n",
        "checkpoint = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
        "del checkpoint[\"optimizer\"]\n",
        "for key in list(checkpoint[\"model\"].keys()):\n",
        "    if \"dvae\" in key:\n",
        "        del checkpoint[\"model\"][key]\n",
        "torch.save(checkpoint, \"model.pth\")\n",
        "model_dir = os.path.dirname(model_path)\n",
        "files.download(os.path.join(model_dir, 'config.json'))\n",
        "files.download(os.path.join(model_dir, 'vocab.json'))\n",
        "files.download('model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copy files to your google drive\n",
        "\n",
        "The two previous cells are a requirement for this step but it can be much faster"
      ],
      "metadata": {
        "id": "Eh9_SusYdRE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "drive.mount('/content/drive')\n",
        "!mkdir /content/drive/MyDrive/XTTS_ft_colab\n",
        "shutil.copy(os.path.join(model_dir, 'config.json'), \"/content/drive/MyDrive/XTTS_ft_colab/config.json\")\n",
        "shutil.copy(os.path.join(model_dir, 'vocab.json'), \"/content/drive/MyDrive/XTTS_ft_colab/vocab.json'\")\n",
        "shutil.copy('model.pth', \"/content/drive/MyDrive/XTTS_ft_colab/model.pth\")\n",
        "shutil.copy('dataset.zip', \"/content/drive/MyDrive/XTTS_ft_colab/dataset.zip\")"
      ],
      "metadata": {
        "id": "piLAaVHSdQs5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cdWKA_xFqkKq",
        "oXEBRA_kq23i",
        "ZKzoP53Nq_rJ",
        "Eh9_SusYdRE4"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}